***
Model Specific README
***

The base implementation of the PPO algorithm used is from [cleanRL](https://github.com/vwxyzjn/cleanrl) the code was then modified to fit within our framework.

***